# ═════════════════════════════════════════════════════════════════════════════
# 🤖 LITELLM CONFIGURATION
# Auto-generated from blueprint-config.json
# ═════════════════════════════════════════════════════════════════════════════

# ═════════════════════════════════════════════════════════════════════════════
# 🤖 MODEL DEFINITIONS
# ═════════════════════════════════════════════════════════════════════════════
model_list:
  # ───────────────────────────────────────────────────────────────────────────
  # CLOUD MODELS (OpenAI, Anthropic, Google, etc.)
  # Auto-generated from blueprint-config.json → litellm.models
  # ───────────────────────────────────────────────────────────────────────────
  
  # gpt-4o (Openai) - GPT-4 Optimized  - model_name: gpt-4o
    litellm_params:
model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
stream: true
  
  # claude-3-5-sonnet-20241022 (Anthropic) - Claude 3.5 Sonnet  - model_name: claude-3-5-sonnet-20241022
    litellm_params:
model: anthropic/claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
stream: true
  
  # ───────────────────────────────────────────────────────────────────────────
  # LOCAL MODELS (via llama-swap → ramalama)
  # Auto-generated from blueprint-config.json → local_inference.models
  # ───────────────────────────────────────────────────────────────────────────
  
  # Qwen 2.5 Coder 7B [coding group] (Local)
  - model_name: qwen2.5-coder:7b
    litellm_params:
      model: openai/qwen2.5-coder:7b
      api_base: http://:/v1
      api_key: "sk-no-key-required"
      stream: true
      custom_llm_provider: openai

  # ───────────────────────────────────────────────────────────────────────
  # EMBEDDING MODELS (via llama-swap → ramalama)
  # ───────────────────────────────────────────────────────────────────────
  
  # Nomic Embed Text [embeddings group] (Local Embedding)
  - model_name: nomic-embed-text
    litellm_params:
      model: openai/nomic-embed-text
      api_base: http://:/v1
      api_key: "sk-no-key-required"
      custom_llm_provider: openai

# ═════════════════════════════════════════════════════════════════════════════
# ⚙️  LITELLM SETTINGS
# ═════════════════════════════════════════════════════════════════════════════
litellm_settings:
  drop_params: true
  
  # Redis caching
  cache: true
  cache_params:
    type: redis
    host: litellm-redis
    port: 6379
    ttl: 3600
  
  set_verbose: false

# ═════════════════════════════════════════════════════════════════════════════
# 🔧 GENERAL SETTINGS
# ═════════════════════════════════════════════════════════════════════════════
general_settings:
  database_url: "postgresql://litellm@litellm-postgres:5432/litellm"
  master_key: os.environ/LITELLM_MASTER_KEY
  store_model_in_db: true
