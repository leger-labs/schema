{% extends "../base-container.njk" %}
{% set catalog = (readFile('../release-catalog.json') | fromJson) -%}
{% set service_def = catalog.services.litellm -%}

{# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #}
{# {{ service_def.name }} v{{ service_def.version }}                          #}
{# Release: {{ catalog.release.version }}                                     #}
{# {{ service_def.notes }}                                                    #}
{# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #}

{# Pass service config to base template #}
{% set service = infrastructure.services.litellm %}
{% set description = service.description %}
{% set image = service_def.image %}
{% set container_name = service.container_name %}
{% set port = service.port %}
{% set published_port = service.published_port %}
{% set bind = service.bind %}

{# Add service-specific dependencies #}
{% block dependencies %}
{% for dep in service.requires %}
After={{ infrastructure.services[dep].container_name }}.service
{% endfor %}
{% endblock %}

{% block wants_dependencies %}
{% for dep in service.requires %}
Wants={{ infrastructure.services[dep].container_name }}.service
{% endfor %}
{% endblock %}

{# Add LiteLLM-specific volumes #}
{% block volumes %}
Volume=%h/.config/containers/systemd/litellm/litellm.yaml:/app/config.yaml:ro,Z
{% endblock %}

{# Add LiteLLM environment variables (non-sensitive only) #}
{% block environment %}
# Database connection
Environment=DATABASE_URL={{ litellm.database_url }}

# Redis connection
Environment=REDIS_HOST={{ infrastructure.services.litellm_redis.hostname }}
Environment=REDIS_PORT={{ infrastructure.services.litellm_redis.port }}

# Settings
Environment=STORE_MODEL_IN_DB=true
Environment=USE_AIOHTTP_TRANSPORT=true
{% endblock %}

{# Secrets block - Uses Podman Secret= directives for secure injection #}
{% block secrets %}
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ” SECRETS - Injected via Podman Secrets (NEVER use Environment= for these)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LiteLLM Master Key
Secret=litellm_master_key,type=env,target=LITELLM_MASTER_KEY

# LLM Provider API Keys (conditionally injected based on usage)
Secret=openai_api_key,type=env,target=OPENAI_API_KEY
Secret=anthropic_api_key,type=env,target=ANTHROPIC_API_KEY
Secret=gemini_api_key,type=env,target=GEMINI_API_KEY
Secret=openrouter_api_key,type=env,target=OPENROUTER_API_KEY
Secret=groq_api_key,type=env,target=GROQ_API_KEY
{% endblock %}

{# Override default health check with LiteLLM-specific endpoint #}
{% block health_check %}
HealthCmd=curl -f http://localhost:{{ port }}/health/readiness || exit 1
HealthInterval=30s
HealthTimeout=10s
HealthRetries=3
HealthStartPeriod=30s
{% endblock %}

{# Add LiteLLM-specific execution parameters #}
{% block service_exec %}
# Add execution command
ExecStart=/usr/bin/podman start {{ container_name }}
ExecStartPre=-/usr/bin/podman stop {{ container_name }}
ExecStartPre=-/usr/bin/podman rm {{ container_name }}

# Log startup info
ExecStartPost=/usr/bin/echo "LiteLLM: Started successfully at http://{{ service.hostname }}:{{ service.port }}"
{% endblock %}

{# Add architecture notes specific to LiteLLM #}
{% block architecture_notes %}
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ ARCHITECTURE NOTES - LiteLLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# DEPENDENCIES:
#   - litellm_postgres (database for model configs)
#   - litellm_redis (caching and rate limiting)
#
# CONFIGURATION:
#   - Models defined in litellm.yaml (mounted as volume)
#   - Database auto-migrates on startup
#   - Supports both cloud and local models via llama-swap
#
# INTEGRATION:
#   - OpenWebUI uses this as primary LLM backend
#   - Jupyter uses this for notebook-intelligence
#   - All task models (title gen, tags gen) route through here
#
# MONITORING:
#   - Health endpoint: /health/readiness
#   - Metrics endpoint: /metrics (if enabled)
#   - Dashboard: https://llm.{{ tailscale.full_hostname }} (via Caddy)
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{% endblock %}
