{% extends "../base-container.njk" %}
{% set catalog = (readFile('../release-catalog.json') | fromJson) -%}
{% set service_def = catalog.services.litellm -%}

{# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #}
{# {{ service_def.name }} v{{ service_def.version }}                          #}
{# Release: {{ catalog.release.version }}                                     #}
{# {{ service_def.notes }}                                                    #}
{# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• #}

{# Pass service config to base template #}
{% set service = infrastructure.services.litellm %}
{% set description = service.description %}
{% set image = service_def.image %}
{% set container_name = service.container_name %}
{% set port = service.port %}
{% set published_port = service.published_port %}
{% set bind = service.bind %}

{# Add service-specific dependencies #}
{% block dependencies %}
{% for dep in service.requires %}
After={{ infrastructure.services[dep].container_name }}.service
{% endfor %}
{% endblock %}

{% block wants_dependencies %}
{% for dep in service.requires %}
Wants={{ infrastructure.services[dep].container_name }}.service
{% endfor %}
{% endblock %}

{# Add LiteLLM-specific volumes #}
{% block volumes %}
Volume=%h/.config/containers/systemd/litellm/litellm.yaml:/app/config.yaml:ro,Z
{% endblock %}

{# Add LiteLLM environment variables #}
{% block environment %}
# Database connection
Environment=DATABASE_URL={{ litellm.database_url }}

# Redis connection
Environment=REDIS_HOST={{ infrastructure.services.litellm_redis.hostname }}
Environment=REDIS_PORT={{ infrastructure.services.litellm_redis.port }}

# Settings
Environment=STORE_MODEL_IN_DB=true
Environment=USE_AIOHTTP_TRANSPORT=true

# Internal Master Key (hardcoded, not a real secret - just for OpenWebUI auth)
Environment=LITELLM_MASTER_KEY=sk-1234
{% endblock %}

{# Secrets block - Auto-generated based on resolved cloud models #}
{% block secrets %}
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ” EXTERNAL API SECRETS
# Auto-generated based on resolved cloud models
# Actual secret values synced via: leger secrets sync
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{# Check which providers are used and generate corresponding Secret= directives #}
{% set has_openai = false %}
{% set has_anthropic = false %}
{% set has_gemini = false %}
{% set has_groq = false %}
{% set has_mistral = false %}
{% set has_openrouter = false %}
{% set has_cohere = false %}
{% set has_deepseek = false %}
{% set has_perplexity = false %}

{% for model in litellm.models %}
  {% if model.provider == 'openai' %}{% set has_openai = true %}{% endif %}
  {% if model.provider == 'anthropic' %}{% set has_anthropic = true %}{% endif %}
  {% if model.provider == 'gemini' %}{% set has_gemini = true %}{% endif %}
  {% if model.provider == 'groq' %}{% set has_groq = true %}{% endif %}
  {% if model.provider == 'mistral' %}{% set has_mistral = true %}{% endif %}
  {% if model.provider == 'openrouter' %}{% set has_openrouter = true %}{% endif %}
  {% if model.provider == 'cohere' %}{% set has_cohere = true %}{% endif %}
  {% if model.provider == 'deepseek' %}{% set has_deepseek = true %}{% endif %}
  {% if model.provider == 'perplexity' %}{% set has_perplexity = true %}{% endif %}
{% endfor %}

{% if has_openai %}
Secret=openai_api_key,type=env,target=OPENAI_API_KEY
{% endif %}
{% if has_anthropic %}
Secret=anthropic_api_key,type=env,target=ANTHROPIC_API_KEY
{% endif %}
{% if has_gemini %}
Secret=gemini_api_key,type=env,target=GEMINI_API_KEY
{% endif %}
{% if has_groq %}
Secret=groq_api_key,type=env,target=GROQ_API_KEY
{% endif %}
{% if has_mistral %}
Secret=mistral_api_key,type=env,target=MISTRAL_API_KEY
{% endif %}
{% if has_openrouter %}
Secret=openrouter_api_key,type=env,target=OPENROUTER_API_KEY
{% endif %}
{% if has_cohere %}
Secret=cohere_api_key,type=env,target=COHERE_API_KEY
{% endif %}
{% if has_deepseek %}
Secret=deepseek_api_key,type=env,target=DEEPSEEK_API_KEY
{% endif %}
{% if has_perplexity %}
Secret=perplexity_api_key,type=env,target=PERPLEXITY_API_KEY
{% endif %}
{% endblock %}

{# Override default health check with LiteLLM-specific endpoint #}
{% block health_check %}
HealthCmd=curl -f http://localhost:{{ port }}/health/readiness || exit 1
HealthInterval=30s
HealthTimeout=10s
HealthRetries=3
HealthStartPeriod=30s
{% endblock %}

{# Add LiteLLM-specific execution parameters #}
{% block service_exec %}
# Add execution command
ExecStart=/usr/bin/podman start {{ container_name }}
ExecStartPre=-/usr/bin/podman stop {{ container_name }}
ExecStartPre=-/usr/bin/podman rm {{ container_name }}

# Log startup info
ExecStartPost=/usr/bin/echo "LiteLLM: Started successfully at http://{{ service.hostname }}:{{ service.port }}"
{% endblock %}

{# Add architecture notes specific to LiteLLM #}
{% block architecture_notes %}
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“ ARCHITECTURE NOTES - LiteLLM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# DEPENDENCIES:
#   - litellm_postgres (database for model configs)
#   - litellm_redis (caching and rate limiting)
#
# CONFIGURATION:
#   - Models defined in litellm.yaml (mounted as volume)
#   - Database auto-migrates on startup
#   - Supports both cloud and local models via llama-swap
#
# INTEGRATION:
#   - OpenWebUI uses this as primary LLM backend
#   - Jupyter uses this for notebook-intelligence
#   - All task models (title gen, tags gen) route through here
#
# MONITORING:
#   - Health endpoint: /health/readiness
#   - Metrics endpoint: /metrics (if enabled)
#   - Dashboard: https://llm.{{ tailscale.full_hostname }} (via Caddy)
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
{% endblock %}
